"""
Evaluation service for the Financial QA chatbot.

This service evaluates the performance of the chatbot across different configurations
(retrieval profiles and models) using standardized metrics.
"""

import json
import time
import asyncio
import uuid
import re
from datetime import datetime, date
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import os
import random

from app.db.pinecone_db import query_pinecone, initialize_pinecone
from app.db.firestore import add_document, get_documents, query_documents, initialize_firestore
from app.services.chat_service import add_chat_message
from app.models.dto import ChatRequestDTO
from app.config import DEFAULT_MODEL, AVAILABLE_MODELS, RETRIEVAL_PROFILES, DATA_DIR
from app.utils.logging_utils import logger

# Constants
MAX_CONCURRENT_EVALUATIONS = 5  # Limit concurrent evaluations to avoid rate limiting
MAX_ACTIVE_EVALUATIONS = 3  # Maximum number of concurrent evaluation runs

# Global state
active_evaluations = 0
_evaluations_lock = asyncio.Lock()

class EvaluationMetrics:
    """
    Class for calculating evaluation metrics for QA pairs in financial contexts.
    
    This class provides methods to evaluate the quality of answers generated by the Financial QA chatbot
    against ground truth answers. It includes metrics for answer relevance, numerical accuracy,
    and citation quality.
    
    Metrics Overview:
    - has_direct_answer: Whether the model provides a direct answer (1) or not (0)
    - has_citations: Whether the answer includes citations to sources (1) or not (0)
    - answer_relevance: How relevant the answer is to the question (0.0-1.0)
    - numerical_accuracy: Whether numerical values match within tolerance (1) or not (0)
    - financial_accuracy: Whether financial values match within stricter tolerance (1) or not (0)
    - partial_numerical_match: Degree of numerical similarity on a continuous scale (0.0-1.0)
    - has_calculation_steps: For calculation questions, whether steps are shown (1) or not (0)
    - is_error_response: Whether the response was an error or failed to retrieve context (1) or not (0)
    """
    
    @staticmethod
    def calculate_metrics(question_type: str, predicted_answer: str, ground_truth_answer: str) -> Dict[str, Any]:
        """
        Calculate metrics for a QA pair.
        
        Args:
            question_type: The type of question (extraction, calculation, etc.)
            predicted_answer: The predicted answer
            ground_truth_answer: The ground truth answer
            
        Returns:
            A dictionary of metrics with the following keys:
            - has_direct_answer: Binary (0 or 1) indicating if the model provided a direct answer
            - has_citations: Binary (0 or 1) indicating if the answer includes citations
            - answer_relevance: Float (0.0-1.0) measuring how relevant the answer is
            - numerical_accuracy: Binary (0 or 1) indicating if numerical values match within tolerance
            - financial_accuracy: Binary (0 or 1) indicating if financial values match within stricter tolerance
            - partial_numerical_match: Float (0.0-1.0) measuring degree of numerical similarity
            - has_calculation_steps: Binary (0 or 1) indicating if calculation steps are shown (for calculation questions)
            - is_error_response: Binary (0 or 1) indicating if the response was an error or failed to retrieve context
        """
        metrics = {}
        
        # Check if the response is an error or failed to retrieve context
        # is_error_response: Binary (0 or 1)
        # Indicates whether the response was an error message or failed to retrieve relevant context
        # 1 = Error response, 0 = Valid response
        error_patterns = [
            r"error",
            r"i'm sorry",
            r"i am sorry",
            r"couldn't process",
            r"could not process",
            r"couldn't find",
            r"could not find",
            r"no relevant",
            r"no information",
            r"don't have",
            r"do not have",
            r"unable to",
            r"failed to",
            r"try again",
            r"context length exceeded"
        ]
        
        is_error = False
        for pattern in error_patterns:
            if re.search(pattern, predicted_answer.lower()):
                is_error = True
                break
        
        metrics["is_error_response"] = 1 if is_error else 0
        
        # Basic metrics
        # has_direct_answer: Binary (0 or 1)
        # Indicates whether the model provided a direct answer rather than saying "I don't know" or similar
        # 1 = Direct answer provided, 0 = No direct answer
        metrics["has_direct_answer"] = 1 if len(predicted_answer) > 0 and "I don't know" not in predicted_answer.lower() and "no information" not in predicted_answer.lower() else 0

        # Check for citations - look for common citation patterns
        # has_citations: Binary (0 or 1)
        # Indicates whether the answer includes citations to sources
        # 1 = Citations present, 0 = No citations
        citation_patterns = [
            r'\[\d+\]',           # [1], [2], etc.
            r'\(\d+\)',           # (1), (2), etc.
            r'reference\s+\d+',   # reference 1, reference 2, etc.
            r'source\s+\d+',      # source 1, source 2, etc.
            r'citation\s+\d+',    # citation 1, citation 2, etc.
            r'according to',      # according to...
            r'mentioned in',      # mentioned in...
            r'stated in',         # stated in...
            r'as per',            # as per...
            r'cited in',          # cited in...
            r'from the',          # from the...
            r'in the document',   # in the document...
            r'document states'    # document states...
        ]

        metrics["has_citations"] = 0
        for pattern in citation_patterns:
            if re.search(pattern, predicted_answer.lower()):
                metrics["has_citations"] = 1
                break
        
        # Add answer relevance score (0-1)
        # answer_relevance: Float (0.0-1.0)
        # Measures how relevant the answer is to the question
        # 0.0 = Not relevant at all, 1.0 = Highly relevant
        metrics["answer_relevance"] = EvaluationMetrics._calculate_relevance_score(predicted_answer)
        
        # For calculation questions, check if it shows steps
        # has_calculation_steps: Binary (0 or 1)
        # For calculation questions, indicates whether the answer shows the calculation steps
        # 1 = Calculation steps shown, 0 = No calculation steps
        if question_type == "calculation":
            metrics["has_calculation_steps"] = 1 if EvaluationMetrics._has_calculation_steps(predicted_answer) else 0
        else:
            # For non-calculation questions, set to 0
            metrics["has_calculation_steps"] = 0
        
        # For numerical questions, calculate numerical metrics
        if EvaluationMetrics._is_numerical(ground_truth_answer):
            # Extract numerical values
            predicted_value = EvaluationMetrics._extract_numerical_value(predicted_answer)
            ground_truth_value = EvaluationMetrics._extract_numerical_value(ground_truth_answer)
            
            # Normalize values for comparison
            norm_predicted = EvaluationMetrics._normalize_number_for_comparison(predicted_value)
            norm_ground_truth = EvaluationMetrics._normalize_number_for_comparison(ground_truth_value)
            
            logger.debug(f"Comparing numerical values: predicted={predicted_value} ({norm_predicted}), ground_truth={ground_truth_value} ({norm_ground_truth})")
            
            if norm_predicted is not None and norm_ground_truth is not None:
                # Check for exact numerical match after normalization
                is_exact_match = (norm_predicted == norm_ground_truth)
                
                # Calculate relative difference for partial matching
                relative_diff = 0.0
                if norm_ground_truth == 0:
                    is_close = (abs(norm_predicted) < 0.001)
                else:
                    relative_diff = abs(norm_predicted - norm_ground_truth) / abs(norm_ground_truth)
                    # Consider values close if they're within 1% of each other
                    is_close = (relative_diff < 0.01)
                
                # Combined check for equality
                is_equal = is_exact_match or is_close
                
                logger.debug(f"Numerical comparison: is_exact_match={is_exact_match}, relative_diff={relative_diff}, is_equal={is_equal}")
                
                # Calculate numerical accuracy (1 if equal or very close, 0 otherwise)
                # numerical_accuracy: Binary (0 or 1)
                # Indicates whether the numerical values match within tolerance (1% or exact match)
                # 1 = Numerical values match, 0 = Numerical values don't match
                metrics["numerical_accuracy"] = 1 if is_equal else 0
                
                # Add partial numerical match (0-1 scale)
                # partial_numerical_match: Float (0.0-1.0)
                # Measures the degree of similarity between numerical values on a continuous scale
                # 1.0 = Perfect match, 0.9-1.0 = Very close, 0.5-0.9 = Moderately close, 0.0-0.5 = Not close
                if is_exact_match:
                    # If the values are exactly equal after normalization, give a perfect score
                    partial_score = 1.0
                elif is_close:
                    # If the values are very close, give a high score
                    partial_score = 0.9 + ((0.01 - relative_diff) * 10)  # 0.9-1.0 range for close matches
                else:
                    # Calculate a similarity score based on relative difference
                    try:
                        # More lenient scale for partial matching
                        if relative_diff <= 0.05:  # 5%
                            partial_score = 0.7 + ((0.05 - relative_diff) * 4)  # 0.7-0.9 range
                        elif relative_diff <= 0.1:  # 10%
                            partial_score = 0.5 + ((0.1 - relative_diff) * 4)   # 0.5-0.7 range
                        elif relative_diff <= 0.5:  # 50%
                            partial_score = 0.2 + ((0.5 - relative_diff) * 0.75)  # 0.2-0.5 range
                        else:
                            partial_score = max(0.1, 0.2 - (relative_diff - 0.5) * 0.1)  # 0.1-0.2 range
                    except Exception as e:
                        logger.error(f"Error calculating partial numerical match: {str(e)}")
                        partial_score = 0.0
                
                metrics["partial_numerical_match"] = partial_score
                
                # For financial questions, calculate financial accuracy
                # financial_accuracy: Binary (0 or 1)
                # Indicates whether financial values match within a stricter tolerance
                # 1 = Financial values match, 0 = Financial values don't match
                metrics["financial_accuracy"] = 1 if is_equal else 0
            else:
                metrics["numerical_accuracy"] = 0
                metrics["partial_numerical_match"] = 0
                metrics["financial_accuracy"] = 0
        else:
            # For non-numerical questions, set numerical metrics to 0
            metrics["numerical_accuracy"] = 0
            metrics["partial_numerical_match"] = 0
            metrics["financial_accuracy"] = 0
        
        return metrics
    
    @staticmethod
    def _calculate_relevance_score(answer: str) -> float:
        """
        Calculate a relevance score for the answer (0-1).
        
        This metric measures how relevant the answer is to a financial question,
        based on the presence of financial terminology and the substantiveness of the response.
        
        Args:
            answer: The predicted answer text
            
        Returns:
            Float between 0.0 and 1.0:
            - 0.0: Not relevant at all (very short or error message)
            - 0.1: Error message or apology
            - 0.3: "No information" type of response
            - 0.5-1.0: Substantive answer with financial terms (higher score for more financial terms)
        """
        # Simple heuristic based on answer length and content
        if len(answer) < 10:
            return 0.0
            
        if "no information" in answer.lower() or "don't have" in answer.lower():
            return 0.3
            
        if "I'm sorry" in answer or "error" in answer.lower():
            return 0.1
            
        # Check for specific financial terms that indicate relevance
        financial_terms = ["million", "billion", "percent", "increase", "decrease", "$", "usd", "revenue", "profit", "loss", "assets", "liabilities"]
        term_count = sum(1 for term in financial_terms if term in answer.lower())
        term_score = min(0.5, term_count * 0.1)  # Max 0.5 from terms
        
        # Base score for having a substantive answer
        base_score = 0.5
        
        return min(1.0, base_score + term_score)
    
    @staticmethod
    def _has_calculation_steps(answer: str) -> bool:
        """
        Check if the answer contains calculation steps.
        
        This metric identifies whether the answer shows the steps taken to arrive at a calculation,
        which is important for calculation-type questions.
        
        Args:
            answer: The predicted answer text
            
        Returns:
            Boolean:
            - True: Answer contains calculation steps (mathematical operators, formulas, etc.)
            - False: Answer does not contain calculation steps
        """
        calculation_indicators = ["+", "-", "*", "/", "=", "sum", "total", "calculate", "computation", "formula"]
        return any(indicator in answer for indicator in calculation_indicators)
    
    @staticmethod
    def _is_numerical(text: str) -> bool:
        """
        Check if the text represents a numerical value.
        
        This helper method determines if a text contains a numerical value that can be extracted.
        
        Args:
            text: The text to check
            
        Returns:
            Boolean:
            - True: Text contains a numerical value
            - False: Text does not contain a numerical value
        """
        # Try to extract a numerical value
        return EvaluationMetrics._extract_numerical_value(text) is not None
    
    @staticmethod
    def _extract_numerical_value(text: str) -> Optional[float]:
        """
        Extract a numerical value from text.
        
        This helper method extracts numerical values from text, handling various formats
        including commas, currency symbols, and multipliers (million, billion).
        
        Args:
            text: The text to extract a numerical value from
            
        Returns:
            Float: The extracted numerical value, or None if no numerical value could be extracted
        """
        try:
            if not text:
                return None
            
            # Convert to string if not already
            if not isinstance(text, str):
                text = str(text)
            
            # Log the original text for debugging
            logger.debug(f"Extracting numerical value from: '{text}'")
            
            # Remove common financial symbols
            text = text.replace("$", "").replace("USD", "")
            
            # Handle multipliers
            multiplier = 1
            if "million" in text.lower() or " m" in text.lower():
                multiplier = 1_000_000
                text = text.replace("million", "").replace(" m", "")
            elif "billion" in text.lower() or " b" in text.lower():
                multiplier = 1_000_000_000
                text = text.replace("billion", "").replace(" b", "")
            
            # Use regex to find numbers, handling commas properly
            import re
            # Look for numbers with or without commas and decimal points
            matches = re.findall(r"[-+]?[\d,]+\.?\d*", text)
            
            if matches:
                # Remove commas from the matched number
                clean_number = matches[0].replace(",", "")
                result = float(clean_number) * multiplier
                logger.debug(f"Extracted numerical value: {result} from '{matches[0]}'")
                return result
            
            logger.debug(f"No numerical value found in: '{text}'")
            return None
        except Exception as e:
            logger.error(f"Error extracting numerical value from '{text}': {str(e)}")
            return None
    
    @staticmethod
    def _normalize_number_for_comparison(value):
        """
        Normalize a number for comparison by stripping formatting.
        
        This helper method standardizes numerical values for comparison by removing
        formatting elements like commas and spaces.
        
        Args:
            value: The numerical value to normalize
            
        Returns:
            Float: The normalized value as a float, or the original value if normalization fails
        """
        if value is None:
            return None
        
        try:
            # Convert to string if not already
            str_value = str(value)
            
            # Remove commas, spaces, and other formatting
            str_value = str_value.replace(',', '').replace(' ', '')
            
            # Convert back to float
            return float(str_value)
        except Exception as e:
            logger.error(f"Error normalizing number for comparison: {str(e)}")
            return value
    
    @staticmethod
    def _calculate_aggregate_metrics(results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Calculate aggregate metrics from a list of results.
        
        This method computes summary statistics across all evaluation results, including:
        - Error rate and counts
        - Counts of questions by type with error/success breakdowns
        - Non-error metrics (metrics calculated only on valid responses)
        
        The aggregated metrics provide a high-level view of model performance.
        
        Args:
            results: A list of result dictionaries, each containing metrics for a single QA pair
            
        Returns:
            Dictionary with the following structure:
            - total_count: Total number of questions evaluated
            - error_count: Number of error responses
            - error_rate: Percentage of responses that were errors
            - question_type_counts: Detailed breakdown of questions by type
              - For each question type (calculation, extraction, other):
                - total: Total count for this question type
                - error_or_no_context: Count and percentage of errors for this type
                - successful: Count of successful responses and metrics
                  - For calculation questions: has_calculation_steps count
            - non_error_metrics: Metrics calculated only on non-error responses
        """
        if not results:
            return {}
        
        # Initialize metrics
        metrics = {
            "total_count": len(results),
            "error_count": 0,
            "error_rate": 0,
            "question_type_counts": {},
            "non_error_metrics": {
                "total_count": 0,
                "numerical_accuracy": {"total": 0, "count": 0, "average": 0},
                "financial_accuracy": {"total": 0, "count": 0, "average": 0},
                "answer_relevance": {"total": 0, "count": 0, "average": 0},
                "partial_numerical_match": {"total": 0, "count": 0, "average": 0},
                "has_citations": {"total": 0, "count": 0, "average": 0},
                "has_calculation_steps": {"total": 0, "count": 0, "average": 0}
            }
        }
        
        # Initialize question type counts with the new structure
        question_types_seen = set()
        for result in results:
            question_type = result.get("question_type", "other")
            question_types_seen.add(question_type)
            
            if question_type not in metrics["question_type_counts"]:
                metrics["question_type_counts"][question_type] = {
                    "total": 0,
                    "error_or_no_context": {
                        "count": 0,
                        "percentage": 0
                    },
                    "successful": {
                        "count": 0
                    }
                }
                
                # Add has_calculation_steps for calculation type
                if question_type == "calculation":
                    metrics["question_type_counts"][question_type]["successful"]["has_calculation_steps"] = 0
        
        # Calculate metrics for all results
        for result in results:
            result_metrics = result.get("metrics", {})
            question_type = result.get("question_type", "other")
            
            # Increment total count for this question type
            metrics["question_type_counts"][question_type]["total"] += 1
            
            # Check if this is an error response
            is_error = result_metrics.get("is_error_response", 0) == 1
            if is_error:
                metrics["error_count"] += 1
                metrics["question_type_counts"][question_type]["error_or_no_context"]["count"] += 1
            else:
                # Count non-error responses
                metrics["non_error_metrics"]["total_count"] += 1
                metrics["question_type_counts"][question_type]["successful"]["count"] += 1
                
                # For calculation questions, track has_calculation_steps
                if question_type == "calculation" and "has_calculation_steps" in result_metrics:
                    if result_metrics["has_calculation_steps"] == 1:
                        metrics["question_type_counts"][question_type]["successful"]["has_calculation_steps"] += 1
            
            # Update non-error metrics
            for metric_name in ["numerical_accuracy", "financial_accuracy", 
                               "answer_relevance", "partial_numerical_match", "has_citations"]:
                if metric_name in result_metrics:
                    metrics["non_error_metrics"][metric_name]["total"] += result_metrics[metric_name]
                    metrics["non_error_metrics"][metric_name]["count"] += 1
            
            # Handle has_calculation_steps separately for calculation questions
            if question_type == "calculation" and "has_calculation_steps" in result_metrics:
                metrics["non_error_metrics"]["has_calculation_steps"]["total"] += result_metrics["has_calculation_steps"]
                metrics["non_error_metrics"]["has_calculation_steps"]["count"] += 1
        
        # Calculate error rate
        metrics["error_rate"] = metrics["error_count"] / metrics["total_count"] if metrics["total_count"] > 0 else 0
        
        # Calculate error percentages for each question type
        for question_type, type_data in metrics["question_type_counts"].items():
            if type_data["total"] > 0:
                type_data["error_or_no_context"]["percentage"] = type_data["error_or_no_context"]["count"] / type_data["total"]
        
        # Calculate averages for non-error metrics
        for metric_name in ["numerical_accuracy", "financial_accuracy", 
                           "answer_relevance", "partial_numerical_match", "has_citations",
                           "has_calculation_steps"]:
            if metrics["non_error_metrics"][metric_name]["count"] > 0:
                metrics["non_error_metrics"][metric_name]["average"] = (
                    metrics["non_error_metrics"][metric_name]["total"] / 
                    metrics["non_error_metrics"][metric_name]["count"]
                )
        
        return metrics

async def get_test_documents(limit: Optional[int] = None) -> List[Dict[str, Any]]:
    """
    Retrieve test documents from the ground truth JSON file.
    
    Args:
        limit: Optional limit on the number of documents to retrieve
        
    Returns:
        List of test documents with their QA pairs
    """
    try:
        with open(Path(DATA_DIR) / "ground_truth.json", "r", encoding="utf-8") as f:
            ground_truth = json.load(f)
        
        logger.info(f"Loaded ground truth data with {len(ground_truth)} documents")
        
        # Convert to list of documents
        test_docs = []
        for doc_id, qa_pairs in ground_truth.items():
            doc = {
                "id": doc_id,
                "metadata": {
                    "qa_pairs_json": json.dumps(qa_pairs)
                }
            }
            test_docs.append(doc)
        
        # Apply limit if specified
        if limit and limit > 0:
            # Ensure we're limiting the number of documents, not QA pairs
            test_docs = test_docs[:limit]
            logger.info(f"Limited to {limit} test documents as specified")
            
        logger.info(f"Returning {len(test_docs)} test documents from ground truth")
        return test_docs
        
    except Exception as e:
        logger.error(f"Error loading ground truth data: {str(e)}")
        return []

def extract_qa_pairs(docs):
    """
    Extract QA pairs from documents.
    
    Args:
        docs: A document or list of documents to extract QA pairs from
        
    Returns:
        List of QA pairs
    """
    all_qa_pairs = []
    
    # Handle single document case
    if not isinstance(docs, list):
        docs = [docs]
    
    # Process each document
    for doc in docs:
        metadata = doc.get("metadata", {})
        
        # Try to get QA pairs from metadata
        if "qa_pairs_json" in metadata:
            try:
                qa_pairs = json.loads(metadata["qa_pairs_json"])
                if isinstance(qa_pairs, list) and len(qa_pairs) > 0:
                    # Ensure each QA pair has the expected format
                    for qa_pair in qa_pairs:
                        # Make sure the answer is a string
                        if "answer" in qa_pair and not isinstance(qa_pair["answer"], str):
                            qa_pair["answer"] = str(qa_pair["answer"])
                    all_qa_pairs.extend(qa_pairs)
                    continue
            except (json.JSONDecodeError, TypeError) as e:
                logger.error(f"Error parsing qa_pairs_json: {str(e)}")
        
        # If we couldn't extract QA pairs from metadata, try to load directly from ground truth
        try:
            # Load ground truth file
            ground_truth_path = Path(DATA_DIR) / "ground_truth.json"
            
            with open(ground_truth_path, "r", encoding="utf-8") as f:
                ground_truth = json.load(f)
            
            # Get QA pairs for this document
            doc_id = doc.get("id")
            if doc_id in ground_truth:
                qa_pairs = ground_truth[doc_id]
                # Ensure each QA pair has the expected format
                for qa_pair in qa_pairs:
                    # Make sure the answer is a string
                    if "answer" in qa_pair and not isinstance(qa_pair["answer"], str):
                        qa_pair["answer"] = str(qa_pair["answer"])
                all_qa_pairs.extend(qa_pairs)
        except Exception as e:
            logger.error(f"Error loading QA pairs from ground truth: {str(e)}")
    
    return all_qa_pairs

async def evaluate_document(
    doc: Dict[str, Any],
    retrieval_profile: str,
    model: str,
    user_id: str = "evaluation_user"
) -> List[Dict[str, Any]]:
    """
    Evaluate a single test document by running its questions through the chat service.
    
    Args:
        doc: The test document with metadata
        retrieval_profile: The retrieval profile to use
        model: The model to use
        user_id: The user ID to use for the chat service
        
    Returns:
        List of evaluation results for each question in the document
    """
    results = []
    
    # Extract QA pairs from document using the flexible function
    qa_pairs = extract_qa_pairs(doc)
    
    if not qa_pairs:
        logger.warning(f"No QA pairs found in document {doc.get('id')}")
        return results
    
    # Process each question in the document
    for i, qa_pair in enumerate(qa_pairs):
        question = qa_pair.get("question", "")
        ground_truth_answer = qa_pair.get("answer", "")
        
        if not question or not ground_truth_answer:
            logger.warning(f"Missing question or answer in QA pair: {qa_pair}")
            continue
        
        # Determine question type
        question_type = determine_question_type(question)
        
        # Create a chat request
        chat_request = ChatRequestDTO(
            question=question,
            model=model,
            retrieval_profile=retrieval_profile,
            user_id=user_id,
            temperature=0.0,  # Use deterministic temperature for evaluation
            max_tokens=1024
        )
        
        # Measure response time
        start_time = time.time()
        
        try:
            # Get response from chat service
            response = await add_chat_message(chat_request)
            
            # Calculate response time
            response_time = time.time() - start_time
            
            # Extract answer from response
            answer = response.get("answer", "")
            
            # Calculate metrics
            metrics = EvaluationMetrics.calculate_metrics(
                question_type=question_type,
                predicted_answer=answer,
                ground_truth_answer=ground_truth_answer
            )
            
            # Calculate retrieval quality metrics
            retrieval_metrics = {
                "has_context": 1 if response.get("context", []) else 0,
                "context_count": len(response.get("context", [])),
            }
            
            # Check if the retrieved contexts contain keywords from the question
            keywords = extract_keywords(question)
            if keywords and response.get("context", []):
                # Check if any of the contexts contain any of the keywords
                context_text = " ".join([context.get("text", "") for context in response.get("context", [])])
                keyword_matches = [keyword for keyword in keywords if keyword.lower() in context_text.lower()]
                retrieval_metrics["keyword_coverage"] = len(keyword_matches) / len(keywords) if keywords else 0
            
            # Create result object
            result = {
                "document_id": doc.get("id"),
                "question": question,
                "question_type": question_type,
                "predicted_answer": answer,
                "ground_truth_answer": ground_truth_answer,
                "metrics": metrics,
                "retrieval_metrics": retrieval_metrics,
                "response_time": response_time,
                "contexts": response.get("context", []),
                "retrieval_profile": retrieval_profile,
                "model": model,
                "timestamp": datetime.now().isoformat()
            }
            
            results.append(result)
            
        except Exception as e:
            logger.error(f"Error evaluating question '{question}': {str(e)}")
            # Add error result
            results.append({
                "document_id": doc.get("id"),
                "question": question,
                "error": str(e),
                "retrieval_profile": retrieval_profile,
                "model": model,
                "timestamp": datetime.now().isoformat()
            })
    
    return results

async def evaluate_document_with_retry(
    doc: Dict[str, Any],
    retrieval_profile: str,
    model: str,
    user_id: str = "evaluation_user",
    max_retries: int = 3
) -> List[Dict[str, Any]]:
    """
    Evaluate a document with retry logic for transient failures.
    
    Args:
        doc: The test document with metadata
        retrieval_profile: The retrieval profile to use
        model: The model to use
        user_id: The user ID to use for the chat service
        max_retries: Maximum number of retry attempts
        
    Returns:
        List of evaluation results for each question in the document
    """
    for attempt in range(max_retries):
        try:
            return await evaluate_document(doc, retrieval_profile, model, user_id)
        except Exception as e:
            if attempt < max_retries - 1:
                # Use exponential backoff for retries
                wait_time = 2 ** attempt
                logger.warning(f"Retry {attempt+1}/{max_retries} for document {doc.get('id')} after {wait_time}s: {str(e)}")
                await asyncio.sleep(wait_time)
            else:
                logger.error(f"Failed after {max_retries} attempts for document {doc.get('id')}: {str(e)}")
                # Return error result on final failure
                return [{
                    "document_id": doc.get("id"),
                    "error": f"Failed after {max_retries} attempts: {str(e)}",
                    "retrieval_profile": retrieval_profile,
                    "model": model,
                    "timestamp": datetime.now().isoformat()
                }]

async def run_evaluation(evaluation_id: str, retrieval_profile: str, model: str, limit: int = None, user_id: str = "evaluation_user"):
    """
    Run evaluation on the ground truth data.
    
    Args:
        evaluation_id: The ID of the evaluation.
        retrieval_profile: The retrieval profile to use.
        model: The model to use.
        limit: The maximum number of QA pairs to evaluate.
        user_id: The ID of the user who initiated the evaluation.
    """
    try:
        logger.info(f"Starting evaluation {evaluation_id} with retrieval profile {retrieval_profile}, model {model}, limit {limit}")
        
        # Create evaluation document
        evaluation = {
            "id": evaluation_id,
            "status": "running",
            "retrieval_profile": retrieval_profile,
            "model": model,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "user_id": user_id,
            "metrics": {},
            "results": []
        }
        
        # Save initial evaluation document
        save_evaluation(evaluation)
        
        # Get test documents
        test_documents = await get_test_documents()
        
        # Extract QA pairs
        qa_pairs = []
        for doc in test_documents:
            doc_qa_pairs = extract_qa_pairs(doc)
            qa_pairs.extend(doc_qa_pairs)
        
        if limit and limit > 0:
            qa_pairs = qa_pairs[:limit]
            
        logger.info(f"Evaluating {len(qa_pairs)} QA pairs")
        
        # Process each QA pair
        for i, qa_pair in enumerate(qa_pairs):
            try:
                logger.info(f"Processing QA pair {i+1}/{len(qa_pairs)}")
                
                # Get question and expected answer
                question = qa_pair["question"]
                expected_answer = qa_pair["answer"]
                question_type = qa_pair.get("type", determine_question_type(question))
                
                # Generate answer using the chat service
                generated_answer = await generate_answer(question, retrieval_profile, model)
                
                # Calculate metrics for this QA pair
                metrics = EvaluationMetrics.calculate_metrics(
                    question_type=question_type,
                    predicted_answer=generated_answer,
                    ground_truth_answer=expected_answer
                )
                
                # Create result object
                result = {
                    "question": question,
                    "question_type": question_type,
                    "predicted_answer": generated_answer,
                    "ground_truth_answer": expected_answer,
                    "metrics": metrics,
                    "retrieval_profile": retrieval_profile,
                    "model": model,
                    "timestamp": datetime.now().isoformat()
                }
                
                # Add to results
                evaluation["results"].append(result)
                
                # Update evaluation document with partial results
                evaluation["updated_at"] = datetime.now().isoformat()
                save_evaluation(evaluation)
                
            except Exception as e:
                logger.error(f"Error processing QA pair {i+1}: {str(e)}")
                continue
        
        # Calculate aggregate metrics
        evaluation["metrics"] = EvaluationMetrics._calculate_aggregate_metrics(evaluation["results"])
        evaluation["status"] = "completed"
        evaluation["updated_at"] = datetime.now().isoformat()
        
        # Save final evaluation document
        save_evaluation(evaluation)
        
        logger.info(f"Evaluation {evaluation_id} completed")
        return evaluation
        
    except Exception as e:
        logger.error(f"Error running evaluation {evaluation_id}: {str(e)}")
        
        # Update evaluation document with error
        evaluation = {
            "id": evaluation_id,
            "status": "error",
            "error": str(e),
            "updated_at": datetime.now().isoformat()
        }
        save_evaluation(evaluation)
        
        raise e

def save_evaluation(evaluation):
    """
    Save evaluation to a JSON file.
    
    Args:
        evaluation: The evaluation document to save.
    """
    try:
        # Get the project root directory
        project_root = Path(__file__).resolve().parent.parent.parent
        
        # Ensure directory exists
        results_dir = project_root / "app" / "data" / "evaluation_results"
        os.makedirs(results_dir, exist_ok=True)
        
        # Create a copy of the evaluation to modify
        evaluation_to_save = evaluation.copy()
        
        # Remove the results field to save space
        if "results" in evaluation_to_save:
            del evaluation_to_save["results"]
        
        # Save to file
        file_path = results_dir / f"evaluation_{evaluation['id']}.json"
        with open(file_path, 'w') as f:
            json.dump(evaluation_to_save, f, indent=2)
        
        logger.info(f"Saved evaluation {evaluation['id']} to {file_path}")
    except Exception as e:
        logger.error(f"Error saving evaluation {evaluation['id']}: {str(e)}")

async def _start_evaluation_task(evaluation_id, retrieval_profile, model, limit=None, user_id=None):
    """
    Start evaluation task in the background.
    
    Args:
        evaluation_id: The ID of the evaluation.
        retrieval_profile: The retrieval profile to use.
        model: The model to use.
        limit: The maximum number of QA pairs to evaluate.
        user_id: The ID of the user who initiated the evaluation.
    """
    global active_evaluations
    
    # Check if max active evaluations reached
    if active_evaluations >= MAX_ACTIVE_EVALUATIONS:
        logger.error(f"Maximum number of active evaluations reached: {MAX_ACTIVE_EVALUATIONS}")
        return
    
    # Increment active evaluations
    active_evaluations += 1
    
    try:
        # Start evaluation task
        await run_evaluation(evaluation_id, retrieval_profile, model, limit, user_id)
    except Exception as e:
        logger.error(f"Error in evaluation task {evaluation_id}: {str(e)}")
    finally:
        # Decrement active evaluations
        active_evaluations -= 1

def determine_question_type(question: str) -> str:
    """
    Determine the type of question based on its content.
    
    Args:
        question: The question text
        
    Returns:
        The question type (extraction, calculation, or other)
    """
    # Convert to lowercase for case-insensitive matching
    question_lower = question.lower()
    
    # Check for calculation keywords
    calculation_keywords = [
        "calculate", "computation", "what is the", "how much", "percentage", 
        "ratio", "average", "mean", "median", "total", "sum", "difference",
        "increase", "decrease", "change", "growth", "rate", "percent", "%"
    ]
    
    # Check for extraction keywords
    extraction_keywords = [
        "what was", "how many", "how much", "value", "amount", "number",
        "figure", "quantity", "level", "balance", "revenue", "income",
        "expense", "cost", "profit", "loss", "debt", "asset", "liability"
    ]
    
    # Count matches for each type
    calculation_count = sum(1 for keyword in calculation_keywords if keyword in question_lower)
    extraction_count = sum(1 for keyword in extraction_keywords if keyword in question_lower)
    
    # Determine type based on keyword counts
    if calculation_count > extraction_count:
        return "calculation"
    elif extraction_count > 0:
        return "extraction"
    else:
        return "other"

def extract_keywords(text: str) -> List[str]:
    """
    Extract important keywords from text.
    
    Args:
        text: The text to extract keywords from
        
    Returns:
        List of keywords
    """
    # Convert to lowercase
    text_lower = text.lower()
    
    # Split into words
    words = re.findall(r'\b\w+\b', text_lower)
    
    # Remove common stop words
    stop_words = {
        "a", "an", "the", "and", "or", "but", "if", "because", "as", "what",
        "which", "this", "that", "these", "those", "then", "just", "so", "than",
        "such", "when", "who", "how", "where", "why", "is", "are", "was", "were",
        "be", "been", "being", "have", "has", "had", "having", "do", "does", "did",
        "doing", "would", "should", "could", "ought", "i'm", "you're", "he's",
        "she's", "it's", "we're", "they're", "i've", "you've", "we've", "they've",
        "i'd", "you'd", "he'd", "she'd", "we'd", "they'd", "i'll", "you'll",
        "he'll", "she'll", "we'll", "they'll", "isn't", "aren't", "wasn't",
        "weren't", "hasn't", "haven't", "hadn't", "doesn't", "don't", "didn't",
        "won't", "wouldn't", "shan't", "shouldn't", "can't", "cannot", "couldn't",
        "mustn't", "let's", "that's", "who's", "what's", "here's", "there's",
        "when's", "where's", "why's", "how's", "a", "an", "the", "and", "but",
        "if", "or", "because", "as", "until", "while", "of", "at", "by", "for",
        "with", "about", "against", "between", "into", "through", "during",
        "before", "after", "above", "below", "to", "from", "up", "down", "in",
        "out", "on", "off", "over", "under", "again", "further", "then", "once",
        "here", "there", "when", "where", "why", "how", "all", "any", "both",
        "each", "few", "more", "most", "other", "some", "such", "no", "nor",
        "not", "only", "own", "same", "so", "than", "too", "very", "can", "will",
        "just", "should", "now"
    }
    
    # Filter out stop words and short words
    keywords = [word for word in words if word not in stop_words and len(word) > 2]
    
    # Add important financial terms even if they're short
    financial_terms = ["q1", "q2", "q3", "q4", "ytd", "yoy", "roi", "eps", "pe", "ebitda", "cagr"]
    for term in financial_terms:
        if term in text_lower and term not in keywords:
            keywords.append(term)
    
    # Return unique keywords
    return list(set(keywords))

async def get_evaluation(evaluation_id: str) -> Optional[Dict[str, Any]]:
    """
    Get an evaluation by ID from the local JSON file.
    
    Args:
        evaluation_id: The evaluation ID
        
    Returns:
        The evaluation or None if not found
    """
    try:
        # Get the project root directory
        project_root = Path(__file__).resolve().parent.parent.parent
        file_path = project_root / "app" / "data" / "evaluation_results" / f"evaluation_{evaluation_id}.json"
        
        if os.path.exists(file_path):
            with open(file_path, 'r') as f:
                return json.load(f)
        return None
    except Exception as e:
        logger.error(f"Error getting evaluation {evaluation_id}: {str(e)}")
        return None

async def get_evaluation_results(evaluation_id: str) -> List[Dict[str, Any]]:
    """
    Get the results for an evaluation from the local JSON file.
    
    Note: Results are no longer stored in the evaluation file to save space.
    This function is kept for API compatibility but will always return an empty list.
    
    Args:
        evaluation_id: The evaluation ID
        
    Returns:
        Empty list (results are no longer stored)
    """
    return []

async def get_evaluations(limit: int = 10) -> List[Dict[str, Any]]:
    """
    Get a list of evaluations from the local JSON files.
    
    Args:
        limit: Maximum number of evaluations to return
        
    Returns:
        List of evaluations
    """
    try:
        # Get the project root directory
        project_root = Path(__file__).resolve().parent.parent.parent
        results_dir = project_root / "app" / "data" / "evaluation_results"
        
        if not results_dir.exists():
            return []
            
        evaluations = []
        for file_path in results_dir.glob("evaluation_*.json"):
            try:
                with open(file_path, 'r') as f:
                    evaluation = json.load(f)
                    evaluations.append(evaluation)
                    if len(evaluations) >= limit:
                        break
            except Exception as e:
                logger.error(f"Error loading evaluation from {file_path}: {str(e)}")
                
        return evaluations
    except Exception as e:
        logger.error(f"Error getting evaluations: {str(e)}")
        return []

async def compare_evaluations(evaluation_ids: List[str]) -> Dict[str, Any]:
    """
    Compare multiple evaluations from local JSON files.
    
    Args:
        evaluation_ids: List of evaluation IDs to compare
        
    Returns:
        Comparison results
    """
    evaluations = []
    
    for evaluation_id in evaluation_ids:
        evaluation = await get_evaluation(evaluation_id)
        if evaluation:
            # Create a clean copy without full results to save memory
            clean_eval = evaluation.copy()
            if "results" in clean_eval:
                del clean_eval["results"]
            evaluations.append(clean_eval)
    
    if not evaluations:
        return {"error": "No evaluations found"}
    
    # Create comparison object
    comparison = {
        "evaluations": evaluations,
        "metrics_comparison": {}
    }
    
    # Compare error rates
    comparison["metrics_comparison"]["error_rate"] = {}
    for evaluation in evaluations:
        eval_id = evaluation["id"]
        metrics = evaluation.get("metrics", {})
        comparison["metrics_comparison"]["error_rate"][eval_id] = metrics.get("error_rate", 0)
    
    # Compare question type counts with the new structure
    comparison["metrics_comparison"]["question_type_counts"] = {}
    for evaluation in evaluations:
        eval_id = evaluation["id"]
        metrics = evaluation.get("metrics", {})
        comparison["metrics_comparison"]["question_type_counts"][eval_id] = metrics.get("question_type_counts", {})
    
    # Compare non-error metrics
    comparison["metrics_comparison"]["non_error_metrics"] = {}
    
    # Basic metrics to compare
    basic_metrics = [
        "numerical_accuracy", 
        "financial_accuracy",
        "answer_relevance",
        "partial_numerical_match",
        "has_citations",
        "has_calculation_steps"
    ]
    
    for metric in basic_metrics:
        comparison["metrics_comparison"]["non_error_metrics"][metric] = {}
        
        for evaluation in evaluations:
            eval_id = evaluation["id"]
            metrics = evaluation.get("metrics", {})
            non_error_metrics = metrics.get("non_error_metrics", {})
            
            metric_data = non_error_metrics.get(metric, {})
            comparison["metrics_comparison"]["non_error_metrics"][metric][eval_id] = metric_data.get("average", 0)
    
    return comparison

def save_results_to_file(evaluation_id, results, evaluation):
    """
    Save evaluation results to a file.
    
    Args:
        evaluation_id: The ID of the evaluation
        results: The evaluation results
        evaluation: The evaluation document with aggregate metrics
    """
    # Only save completed evaluations
    if evaluation.get("status") != "completed":
        logger.info(f"Skipping saving results for evaluation {evaluation_id} as it's not completed yet")
        return
        
    try:
        # Create the results directory if it doesn't exist
        results_dir = Path("app/data/evaluation_results")
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # Create a results file
        results_file = results_dir / f"evaluation_{evaluation_id}.json"
        
        # Create a copy of the evaluation to save
        evaluation_to_save = evaluation.copy()
        
        # Remove the results field to save space
        if "results" in evaluation_to_save:
            del evaluation_to_save["results"]
        
        # Convert any non-serializable objects to strings
        def json_serializable(obj):
            if isinstance(obj, (datetime, date)):
                return obj.isoformat()
            try:
                json.dumps(obj)
                return obj
            except (TypeError, OverflowError):
                return str(obj)
        
        # Save the evaluation to the file with custom serialization
        with open(results_file, "w") as f:
            json.dump(evaluation_to_save, f, indent=2, default=json_serializable)
            
        logger.info(f"Saved evaluation results to {results_file}")
        
    except Exception as e:
        logger.error(f"Error saving evaluation results to file: {str(e)}")

async def generate_answer(question: str, retrieval_profile: str, model: str) -> str:
    """
    Generate an answer for a question using the chat service.
    
    Args:
        question: The question to answer
        retrieval_profile: The retrieval profile to use
        model: The model to use
        
    Returns:
        The generated answer
    """
    try:
        # Create a conversation ID for this question
        conversation_id = str(uuid.uuid4())
        logger.info(f"Generated new conversation ID: {conversation_id}")
        
        # Create a chat request
        chat_request = ChatRequestDTO(
            question=question,
            model=model,
            retrieval_profile=retrieval_profile,
            user_id="admin@email.com",  # Always use admin user for evaluations
            conversation_id=conversation_id,
            temperature=0.0,  # Use deterministic temperature for evaluation
            max_tokens=1024
        )
        
        # Get response from chat service
        response = await add_chat_message(chat_request)
        
        # Extract answer from response
        answer = response.get("answer", "")
        
        return answer
    except Exception as e:
        logger.error(f"Error generating answer for question '{question}': {str(e)}")
        return ""
